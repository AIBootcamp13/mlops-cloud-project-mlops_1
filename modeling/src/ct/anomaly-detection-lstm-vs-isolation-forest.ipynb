{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context\n",
    "\n",
    "In the Solar Energy Industry it is common to have **misproduction problems** regarding various topics such as dirty solar panels, inverter failures, sensor issues and more. In this Notebook I will compare two approaches. The first one using **Isolation Forest** and the second an **LSTM Autoencoder**, to see which approach is the most efficient to detect anomalies in an AC Power timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generation1 = pd.read_csv('../../../data/train/Plant_1_Generation_Data.csv')\n",
    "weather1 = pd.read_csv('../../../data/train/Plant_1_Weather_Sensor_Data.csv')\n",
    "generation1['DATE_TIME'] = pd.to_datetime(generation1['DATE_TIME'], dayfirst=True)\n",
    "weather1['DATE_TIME'] = pd.to_datetime(weather1['DATE_TIME'], dayfirst=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE_TIME</th>\n",
       "      <th>PLANT_ID</th>\n",
       "      <th>SOURCE_KEY</th>\n",
       "      <th>DC_POWER</th>\n",
       "      <th>AC_POWER</th>\n",
       "      <th>DAILY_YIELD</th>\n",
       "      <th>TOTAL_YIELD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-05-15 00:00:00</td>\n",
       "      <td>4135001</td>\n",
       "      <td>1BY6WEcLGh8j5v7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6259559.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-05-15 00:00:00</td>\n",
       "      <td>4135001</td>\n",
       "      <td>1IF53ai7Xc0U56Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6183645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-05-15 00:00:00</td>\n",
       "      <td>4135001</td>\n",
       "      <td>3PZuoBAID5Wc2HD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6987759.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-05-15 00:00:00</td>\n",
       "      <td>4135001</td>\n",
       "      <td>7JYdWkrLSPkdwr4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7602960.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-05-15 00:00:00</td>\n",
       "      <td>4135001</td>\n",
       "      <td>McdE0feGgRqW7Ca</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7158964.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51921</th>\n",
       "      <td>2020-06-09 23:45:00</td>\n",
       "      <td>4135001</td>\n",
       "      <td>uHbuxQJl8lW7ozc</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8309.000000</td>\n",
       "      <td>7233863.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51922</th>\n",
       "      <td>2020-06-09 23:45:00</td>\n",
       "      <td>4135001</td>\n",
       "      <td>wCURE6d3bPkepu2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3432.857143</td>\n",
       "      <td>6975980.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51923</th>\n",
       "      <td>2020-06-09 23:45:00</td>\n",
       "      <td>4135001</td>\n",
       "      <td>z9Y9gH1T5YWrNuG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7629.000000</td>\n",
       "      <td>7199053.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51924</th>\n",
       "      <td>2020-06-09 23:45:00</td>\n",
       "      <td>4135001</td>\n",
       "      <td>zBIq5rxdHJRwDNY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8136.000000</td>\n",
       "      <td>6531405.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51925</th>\n",
       "      <td>2020-06-09 23:45:00</td>\n",
       "      <td>4135001</td>\n",
       "      <td>zVJPv84UY57bAof</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8289.000000</td>\n",
       "      <td>7310456.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51926 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                DATE_TIME  PLANT_ID       SOURCE_KEY  DC_POWER  AC_POWER  \\\n",
       "0     2020-05-15 00:00:00   4135001  1BY6WEcLGh8j5v7       0.0       0.0   \n",
       "1     2020-05-15 00:00:00   4135001  1IF53ai7Xc0U56Y       0.0       0.0   \n",
       "2     2020-05-15 00:00:00   4135001  3PZuoBAID5Wc2HD       0.0       0.0   \n",
       "3     2020-05-15 00:00:00   4135001  7JYdWkrLSPkdwr4       0.0       0.0   \n",
       "4     2020-05-15 00:00:00   4135001  McdE0feGgRqW7Ca       0.0       0.0   \n",
       "...                   ...       ...              ...       ...       ...   \n",
       "51921 2020-06-09 23:45:00   4135001  uHbuxQJl8lW7ozc       0.0       0.0   \n",
       "51922 2020-06-09 23:45:00   4135001  wCURE6d3bPkepu2       0.0       0.0   \n",
       "51923 2020-06-09 23:45:00   4135001  z9Y9gH1T5YWrNuG       0.0       0.0   \n",
       "51924 2020-06-09 23:45:00   4135001  zBIq5rxdHJRwDNY       0.0       0.0   \n",
       "51925 2020-06-09 23:45:00   4135001  zVJPv84UY57bAof       0.0       0.0   \n",
       "\n",
       "       DAILY_YIELD  TOTAL_YIELD  \n",
       "0         0.000000    6259559.0  \n",
       "1         0.000000    6183645.0  \n",
       "2         0.000000    6987759.0  \n",
       "3         0.000000    7602960.0  \n",
       "4         0.000000    7158964.0  \n",
       "...            ...          ...  \n",
       "51921  8309.000000    7233863.0  \n",
       "51922  3432.857143    6975980.0  \n",
       "51923  7629.000000    7199053.0  \n",
       "51924  8136.000000    6531405.0  \n",
       "51925  8289.000000    7310456.0  \n",
       "\n",
       "[51926 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of inverters 22\n"
     ]
    }
   ],
   "source": [
    "inverters = list(generation1['SOURCE_KEY'].unique())\n",
    "print(f\"total number of inverters {len(inverters)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverter level Anomally detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1BY6WEcLGh8j5v7'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inv_1 = generation1[generation1['SOURCE_KEY']==inverters[0]]\n",
    "mask = ((weather1['DATE_TIME'] >= min(inv_1[\"DATE_TIME\"])) & (weather1['DATE_TIME'] <= max(inv_1[\"DATE_TIME\"])))\n",
    "weather_filtered = weather1.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2414, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=inv_1[\"DATE_TIME\"], y=inv_1[\"AC_POWER\"],\n",
    "                    mode='lines',\n",
    "                    name='AC Power'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=weather_filtered[\"DATE_TIME\"], y=weather_filtered[\"IRRADIATION\"],\n",
    "                    mode='lines',\n",
    "                    name='Irradiation', \n",
    "                    yaxis='y2'))\n",
    "\n",
    "fig.update_layout(title_text=\"Irradiation vs AC POWER\",\n",
    "                  yaxis1=dict(title=\"AC Power in kW\",\n",
    "                              side='left'),\n",
    "                  yaxis2=dict(title=\"Irradiation index\",\n",
    "                              side='right',\n",
    "                              anchor=\"x\",\n",
    "                              overlaying=\"y\"\n",
    "                             ))\n",
    "\n",
    "fig.write_image('../../../data/outputs/train/AC_power.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph observations\n",
    "We can see that in June 7th and June 14th there are some misproduction areas that could be considered anomalies. Due to the fact that energy production should behave in a linear way to irradiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE_TIME</th>\n",
       "      <th>AC_POWER</th>\n",
       "      <th>AMBIENT_TEMPERATURE</th>\n",
       "      <th>MODULE_TEMPERATURE</th>\n",
       "      <th>IRRADIATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-05-15 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.184316</td>\n",
       "      <td>22.857507</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-05-15 00:15:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.084589</td>\n",
       "      <td>22.761668</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-05-15 00:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.935753</td>\n",
       "      <td>22.592306</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-05-15 00:45:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.846130</td>\n",
       "      <td>22.360852</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-05-15 01:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.621525</td>\n",
       "      <td>22.165423</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>2020-06-09 22:45:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.946915</td>\n",
       "      <td>20.554199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>2020-06-09 23:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.883195</td>\n",
       "      <td>20.490859</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>2020-06-09 23:15:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.892314</td>\n",
       "      <td>20.571364</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>2020-06-09 23:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.866815</td>\n",
       "      <td>20.661737</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>2020-06-09 23:45:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.825907</td>\n",
       "      <td>20.686540</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2388 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               DATE_TIME  AC_POWER  AMBIENT_TEMPERATURE  MODULE_TEMPERATURE  \\\n",
       "0    2020-05-15 00:00:00       0.0            25.184316           22.857507   \n",
       "1    2020-05-15 00:15:00       0.0            25.084589           22.761668   \n",
       "2    2020-05-15 00:30:00       0.0            24.935753           22.592306   \n",
       "3    2020-05-15 00:45:00       0.0            24.846130           22.360852   \n",
       "4    2020-05-15 01:00:00       0.0            24.621525           22.165423   \n",
       "...                  ...       ...                  ...                 ...   \n",
       "2383 2020-06-09 22:45:00       0.0            22.946915           20.554199   \n",
       "2384 2020-06-09 23:00:00       0.0            22.883195           20.490859   \n",
       "2385 2020-06-09 23:15:00       0.0            22.892314           20.571364   \n",
       "2386 2020-06-09 23:30:00       0.0            22.866815           20.661737   \n",
       "2387 2020-06-09 23:45:00       0.0            22.825907           20.686540   \n",
       "\n",
       "      IRRADIATION  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             0.0  \n",
       "4             0.0  \n",
       "...           ...  \n",
       "2383          0.0  \n",
       "2384          0.0  \n",
       "2385          0.0  \n",
       "2386          0.0  \n",
       "2387          0.0  \n",
       "\n",
       "[2388 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = inv_1.merge(weather_filtered, on=\"DATE_TIME\", how='left')\n",
    "df = df[['DATE_TIME', 'AC_POWER', 'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Here we can see how the Isolation Forest Model is behaving. The yellow dots show us the anomalies detected on the test dataset as well as the red squares that show us the anomalies detected on the training dataset. These points do not follow the contour pattern of the graph and we can clearly see that the yellow dots on the far left are the points from June 7th and June 14th."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Autoencoder approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = df[[\"DATE_TIME\", \"AC_POWER\", \"AMBIENT_TEMPERATURE\", \"MODULE_TEMPERATURE\", \"IRRADIATION\"]]\n",
    "df_timestamp = df[[\"DATE_TIME\"]]\n",
    "df_ = df[[\"AC_POWER\", \"AMBIENT_TEMPERATURE\", \"MODULE_TEMPERATURE\", \"IRRADIATION\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_prp = .6\n",
    "train = df_.loc[:df_.shape[0]*train_prp]\n",
    "test = df_.loc[df_.shape[0]*train_prp:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1433, 1, 4)\n",
      "X_test shape: (955, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(train)\n",
    "X_test = scaler.transform(test)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = train_test_split(X_train, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, seq_len, n_features):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.n_features = n_features\n",
    "        self.embedding_dim = 4  # same as L2 & L4 unit count\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.LSTM(input_size=n_features, hidden_size=16, batch_first=True),\n",
    "            nn.LSTM(input_size=16, hidden_size=self.embedding_dim, batch_first=True)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.LSTM(input_size=self.embedding_dim, hidden_size=self.embedding_dim, batch_first=True),\n",
    "            nn.LSTM(input_size=self.embedding_dim, hidden_size=16, batch_first=True),\n",
    "            nn.Linear(16, n_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        x, _ = self.encoder[0](x)\n",
    "        x, (hidden, _) = self.encoder[1](x)\n",
    "\n",
    "        # Repeat vector (same as RepeatVector in Keras)\n",
    "        x = hidden.repeat(self.seq_len, 1, 1).permute(1, 0, 2)\n",
    "\n",
    "        # Decode\n",
    "        x, _ = self.decoder[0](x)\n",
    "        x, _ = self.decoder[1](x)\n",
    "        x = self.decoder[2](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] | Train Loss: 0.2357 | Val Loss: 0.2364\n",
      "Epoch [2/100] | Train Loss: 0.2082 | Val Loss: 0.1700\n",
      "Epoch [3/100] | Train Loss: 0.0912 | Val Loss: 0.0720\n",
      "Epoch [4/100] | Train Loss: 0.0673 | Val Loss: 0.0658\n",
      "Epoch [5/100] | Train Loss: 0.0618 | Val Loss: 0.0613\n",
      "Epoch [6/100] | Train Loss: 0.0586 | Val Loss: 0.0589\n",
      "Epoch [7/100] | Train Loss: 0.0565 | Val Loss: 0.0578\n",
      "Epoch [8/100] | Train Loss: 0.0556 | Val Loss: 0.0576\n",
      "Epoch [9/100] | Train Loss: 0.0554 | Val Loss: 0.0564\n",
      "Epoch [10/100] | Train Loss: 0.0548 | Val Loss: 0.0562\n",
      "Epoch [11/100] | Train Loss: 0.0549 | Val Loss: 0.0563\n",
      "Epoch [12/100] | Train Loss: 0.0547 | Val Loss: 0.0563\n",
      "Epoch [13/100] | Train Loss: 0.0544 | Val Loss: 0.0565\n",
      "Epoch [14/100] | Train Loss: 0.0544 | Val Loss: 0.0554\n",
      "Epoch [15/100] | Train Loss: 0.0542 | Val Loss: 0.0553\n",
      "Epoch [16/100] | Train Loss: 0.0540 | Val Loss: 0.0546\n",
      "Epoch [17/100] | Train Loss: 0.0539 | Val Loss: 0.0558\n",
      "Epoch [18/100] | Train Loss: 0.0537 | Val Loss: 0.0543\n",
      "Epoch [19/100] | Train Loss: 0.0536 | Val Loss: 0.0537\n",
      "Epoch [20/100] | Train Loss: 0.0530 | Val Loss: 0.0535\n",
      "Epoch [21/100] | Train Loss: 0.0527 | Val Loss: 0.0534\n",
      "Epoch [22/100] | Train Loss: 0.0524 | Val Loss: 0.0521\n",
      "Epoch [23/100] | Train Loss: 0.0515 | Val Loss: 0.0507\n",
      "Epoch [24/100] | Train Loss: 0.0509 | Val Loss: 0.0501\n",
      "Epoch [25/100] | Train Loss: 0.0499 | Val Loss: 0.0485\n",
      "Epoch [26/100] | Train Loss: 0.0480 | Val Loss: 0.0464\n",
      "Epoch [27/100] | Train Loss: 0.0457 | Val Loss: 0.0442\n",
      "Epoch [28/100] | Train Loss: 0.0428 | Val Loss: 0.0439\n",
      "Epoch [29/100] | Train Loss: 0.0401 | Val Loss: 0.0388\n",
      "Epoch [30/100] | Train Loss: 0.0376 | Val Loss: 0.0363\n",
      "Epoch [31/100] | Train Loss: 0.0361 | Val Loss: 0.0348\n",
      "Epoch [32/100] | Train Loss: 0.0328 | Val Loss: 0.0315\n",
      "Epoch [33/100] | Train Loss: 0.0270 | Val Loss: 0.0238\n",
      "Epoch [34/100] | Train Loss: 0.0196 | Val Loss: 0.0185\n",
      "Epoch [35/100] | Train Loss: 0.0158 | Val Loss: 0.0163\n",
      "Epoch [36/100] | Train Loss: 0.0152 | Val Loss: 0.0163\n",
      "Epoch [37/100] | Train Loss: 0.0145 | Val Loss: 0.0155\n",
      "Epoch [38/100] | Train Loss: 0.0147 | Val Loss: 0.0170\n",
      "Epoch [39/100] | Train Loss: 0.0145 | Val Loss: 0.0157\n",
      "Epoch [40/100] | Train Loss: 0.0140 | Val Loss: 0.0153\n",
      "Epoch [41/100] | Train Loss: 0.0139 | Val Loss: 0.0153\n",
      "Epoch [42/100] | Train Loss: 0.0139 | Val Loss: 0.0153\n",
      "Epoch [43/100] | Train Loss: 0.0138 | Val Loss: 0.0151\n",
      "Epoch [44/100] | Train Loss: 0.0136 | Val Loss: 0.0146\n",
      "Epoch [45/100] | Train Loss: 0.0134 | Val Loss: 0.0146\n",
      "Epoch [46/100] | Train Loss: 0.0134 | Val Loss: 0.0153\n",
      "Epoch [47/100] | Train Loss: 0.0133 | Val Loss: 0.0149\n",
      "Epoch [48/100] | Train Loss: 0.0134 | Val Loss: 0.0145\n",
      "Epoch [49/100] | Train Loss: 0.0131 | Val Loss: 0.0157\n",
      "Epoch [50/100] | Train Loss: 0.0132 | Val Loss: 0.0143\n",
      "Epoch [51/100] | Train Loss: 0.0133 | Val Loss: 0.0144\n",
      "Epoch [52/100] | Train Loss: 0.0128 | Val Loss: 0.0142\n",
      "Epoch [53/100] | Train Loss: 0.0131 | Val Loss: 0.0140\n",
      "Epoch [54/100] | Train Loss: 0.0127 | Val Loss: 0.0144\n",
      "Epoch [55/100] | Train Loss: 0.0128 | Val Loss: 0.0135\n",
      "Epoch [56/100] | Train Loss: 0.0128 | Val Loss: 0.0137\n",
      "Epoch [57/100] | Train Loss: 0.0127 | Val Loss: 0.0141\n",
      "Epoch [58/100] | Train Loss: 0.0126 | Val Loss: 0.0139\n",
      "Epoch [59/100] | Train Loss: 0.0127 | Val Loss: 0.0143\n",
      "Epoch [60/100] | Train Loss: 0.0125 | Val Loss: 0.0135\n",
      "Epoch [61/100] | Train Loss: 0.0125 | Val Loss: 0.0136\n",
      "Epoch [62/100] | Train Loss: 0.0125 | Val Loss: 0.0143\n",
      "Epoch [63/100] | Train Loss: 0.0124 | Val Loss: 0.0137\n",
      "Epoch [64/100] | Train Loss: 0.0125 | Val Loss: 0.0133\n",
      "Epoch [65/100] | Train Loss: 0.0123 | Val Loss: 0.0134\n",
      "Epoch [66/100] | Train Loss: 0.0122 | Val Loss: 0.0139\n",
      "Epoch [67/100] | Train Loss: 0.0124 | Val Loss: 0.0135\n",
      "Epoch [68/100] | Train Loss: 0.0122 | Val Loss: 0.0143\n",
      "Epoch [69/100] | Train Loss: 0.0122 | Val Loss: 0.0137\n",
      "Epoch [70/100] | Train Loss: 0.0120 | Val Loss: 0.0132\n",
      "Epoch [71/100] | Train Loss: 0.0121 | Val Loss: 0.0133\n",
      "Epoch [72/100] | Train Loss: 0.0121 | Val Loss: 0.0133\n",
      "Epoch [73/100] | Train Loss: 0.0119 | Val Loss: 0.0130\n",
      "Epoch [74/100] | Train Loss: 0.0121 | Val Loss: 0.0137\n",
      "Epoch [75/100] | Train Loss: 0.0121 | Val Loss: 0.0132\n",
      "Epoch [76/100] | Train Loss: 0.0121 | Val Loss: 0.0130\n",
      "Epoch [77/100] | Train Loss: 0.0121 | Val Loss: 0.0134\n",
      "Epoch [78/100] | Train Loss: 0.0121 | Val Loss: 0.0132\n",
      "Epoch [79/100] | Train Loss: 0.0121 | Val Loss: 0.0132\n",
      "Epoch [80/100] | Train Loss: 0.0120 | Val Loss: 0.0135\n",
      "Epoch [81/100] | Train Loss: 0.0122 | Val Loss: 0.0132\n",
      "Epoch [82/100] | Train Loss: 0.0120 | Val Loss: 0.0130\n",
      "Epoch [83/100] | Train Loss: 0.0119 | Val Loss: 0.0130\n",
      "Epoch [84/100] | Train Loss: 0.0120 | Val Loss: 0.0136\n",
      "Epoch [85/100] | Train Loss: 0.0122 | Val Loss: 0.0132\n",
      "Epoch [86/100] | Train Loss: 0.0118 | Val Loss: 0.0128\n",
      "Epoch [87/100] | Train Loss: 0.0124 | Val Loss: 0.0131\n",
      "Epoch [88/100] | Train Loss: 0.0117 | Val Loss: 0.0129\n",
      "Epoch [89/100] | Train Loss: 0.0118 | Val Loss: 0.0130\n",
      "Epoch [90/100] | Train Loss: 0.0119 | Val Loss: 0.0133\n",
      "Epoch [91/100] | Train Loss: 0.0118 | Val Loss: 0.0133\n",
      "Epoch [92/100] | Train Loss: 0.0120 | Val Loss: 0.0132\n",
      "Epoch [93/100] | Train Loss: 0.0117 | Val Loss: 0.0127\n",
      "Epoch [94/100] | Train Loss: 0.0117 | Val Loss: 0.0138\n",
      "Epoch [95/100] | Train Loss: 0.0121 | Val Loss: 0.0134\n",
      "Epoch [96/100] | Train Loss: 0.0120 | Val Loss: 0.0132\n",
      "Epoch [97/100] | Train Loss: 0.0117 | Val Loss: 0.0131\n",
      "Epoch [98/100] | Train Loss: 0.0119 | Val Loss: 0.0129\n",
      "Epoch [99/100] | Train Loss: 0.0120 | Val Loss: 0.0133\n",
      "Epoch [100/100] | Train Loss: 0.0119 | Val Loss: 0.0132\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = LSTMAutoencoder(seq_len=X_train.shape[1], n_features=X_train.shape[2])\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch[0]  # unpack from TensorDataset\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_epoch_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch[0]\n",
    "            output = model(batch)\n",
    "            loss = criterion(output, batch)\n",
    "            val_epoch_loss += loss.item()\n",
    "    avg_val_loss = val_epoch_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../../../data/outputs/train/lstm.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(len(train_losses))),\n",
    "    y=train_losses,\n",
    "    mode='lines',\n",
    "    name='Train Loss'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(len(val_losses))),\n",
    "    y=val_losses,\n",
    "    mode='lines',\n",
    "    name='Validation Loss'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Autoencoder MAE Loss over Epochs\",\n",
    "    xaxis=dict(title=\"Epoch\"),\n",
    "    yaxis=dict(title=\"MAE Loss\"),\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.write_image('../../../data/outputs/train/Error_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 예측 (no grad & eval mode)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_pred = model(X_tensor)\n",
    "    X_pred = X_pred.detach().numpy()\n",
    "\n",
    "# reshape: (batch, seq_len, features) → (batch, features)\n",
    "X_pred = X_pred[:, -1, :]  # 가장 마지막 시점의 예측값 사용\n",
    "\n",
    "# inverse scaling\n",
    "X_pred_inv = scaler.inverse_transform(X_pred)\n",
    "\n",
    "# DataFrame 생성\n",
    "X_pred_df = pd.DataFrame(X_pred_inv, columns=train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scores = pd.DataFrame()\n",
    "scores['AC_train'] = train['AC_POWER'].values[:len(X_pred_df)]\n",
    "scores['AC_predicted'] = X_pred_df['AC_POWER']\n",
    "scores['loss_mae'] = (scores['AC_train'] - scores['AC_predicted']).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Histogram(x=scores['loss_mae'])])\n",
    "fig.update_layout(title=\"Error distribution\", \n",
    "                 xaxis=dict(title=\"Error delta between predicted and real data [AC Power]\"),\n",
    "                 yaxis=dict(title=\"Data point counts\"))\n",
    "fig.write_image('../../../data/outputs/train/Error_distribution.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    X_pred = model(X_tensor).detach().numpy()\n",
    "\n",
    "X_pred = X_pred[:, -1, :]  # (batch, features)\n",
    "X_pred_inv = scaler.inverse_transform(X_pred)\n",
    "X_pred_df = pd.DataFrame(X_pred_inv, columns=train.columns)\n",
    "X_pred_df.index = test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 예측 결과 DataFrame 복사\n",
    "scores = X_pred_df.copy()\n",
    "\n",
    "# datetime 컬럼 추가 (예: sliding window 적용 시 1893부터)\n",
    "scores['datetime'] = df_timestamp.loc[scores.index]\n",
    "\n",
    "# 실제값 추가\n",
    "scores['real AC'] = test['AC_POWER'].values\n",
    "\n",
    "# MAE 계산\n",
    "scores['loss_mae'] = (scores['real AC'] - scores['AC_POWER']).abs()\n",
    "\n",
    "# 이상치 기준 설정\n",
    "scores['Threshold'] = 200\n",
    "scores['Anomaly'] = np.where(scores['loss_mae'] > scores['Threshold'], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=scores['datetime'], \n",
    "                         y=scores['loss_mae'], \n",
    "                         name=\"Loss\"))\n",
    "fig.add_trace(go.Scatter(x=scores['datetime'], \n",
    "                         y=scores['Threshold'],\n",
    "                         name=\"Threshold\"))\n",
    "\n",
    "fig.update_layout(title=\"Error Timeseries and Threshold\", \n",
    "                 xaxis=dict(title=\"DateTime\"),\n",
    "                 yaxis=dict(title=\"Loss\"))\n",
    "fig.write_image('../../../data/outputs/train/Threshold.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Anomaly\n",
       "0    939\n",
       "1     16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['Anomaly'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "anomalies = scores[scores['Anomaly'] == 1][['real AC']]\n",
    "anomalies = anomalies.rename(columns={'real AC':'anomalies'})\n",
    "scores = scores.merge(anomalies, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[(scores['Anomaly']==1)&(scores['datetime'].notnull())].to_csv('../../../data/outputs/train/anomalies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=scores[\"datetime\"], y=scores[\"real AC\"],\n",
    "                    mode='lines',\n",
    "                    name='AC Power'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=scores[\"datetime\"], y=scores[\"anomalies\"],\n",
    "                    name='Anomaly', \n",
    "                    mode='markers',\n",
    "                    marker=dict(color=\"red\",\n",
    "                                size=11,\n",
    "                                line=dict(color=\"red\",\n",
    "                                          width=2))))\n",
    "\n",
    "fig.update_layout(title_text=\"Anomalies Detected LSTM Autoencoder\")\n",
    "\n",
    "fig.write_image('../../../data/outputs/train/Anomaly.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We see that the LSTM Autoencoder approach is a more efficient way to detect anomalies, againts the Isolation Forest approach, perhaps with a larger dataset the Isolation tree could outperform the Autoencoder, having a faster and pretty good model to detect anomalies. \n",
    "\n",
    "We can see from the Isolation Forest graph how the model is detecting anomalies, highlighting the datapoints from June 7th and June 14th.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 836676,
     "sourceId": 1428586,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30042,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
